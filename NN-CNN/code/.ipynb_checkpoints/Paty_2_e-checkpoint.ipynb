{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import collections\n",
    "import pickle\n",
    "import os, sys, time\n",
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "import numpy.matlib\n",
    "\n",
    "def save_model(class_dict):\n",
    "    file_obj = open(\"../models/NP_CNN.obj\",\"wb\")\n",
    "    pickle.dump(class_dict, file_obj)\n",
    "    file_obj.close()\n",
    "    print(\"model saved to {}\".format('../models/NP_CNN.obj'))\n",
    "\n",
    "def load_model():\n",
    "    file_obj = open(\"../models/NP_CNN.obj\",\"rb\")\n",
    "    class_dict = pickle.load(file_obj)\n",
    "    file_obj.close()\n",
    "    print(\"loaded model {}\".format('../models/NP_CNN.obj'))\n",
    "    return class_dict\n",
    "\n",
    "def save_learning_curve(train_error_list, test_error_list, train_loss_list, test_loss_list):\n",
    "    np.save(\"../models/NP_CNN_train_error_list.npy\", train_error_list)\n",
    "    np.save(\"../models/NP_CNN_test_error_list.npy\", test_error_list)\n",
    "    np.save(\"../models/NP_CNN_train_loss_list.npy\", train_loss_list)\n",
    "    np.save(\"../models/NP_CNN_test_loss_list.npy\", test_loss_list)\n",
    "    print(\"data saved to {}\".format('../models/'))\n",
    "\n",
    "def robust_log(input_arg):\n",
    "    if input_arg >= math.pow(10,-300):\n",
    "        return math.log(input_arg)\n",
    "    else:\n",
    "        return -70000\n",
    "\n",
    "class linear_layer():\n",
    "    #Initialization\n",
    "    def __init__(self,num_input,num_node):\n",
    "        self.num_input = num_input\n",
    "        self.num_node = num_node\n",
    "        b = 0.01*np.ones((1,num_node))\n",
    "        W_initial = np.random.normal(size=[num_input,num_node])\n",
    "        self.W = np.concatenate((W_initial,b),axis=0)\n",
    "        self.dldW = collections.defaultdict(float)\n",
    "        self.x = collections.defaultdict(float)\n",
    "        self.y = collections.defaultdict(float)\n",
    "        self.dldb = collections.defaultdict(float)\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self,x):\n",
    "        x = np.asarray(x)\n",
    "        xstar = np.concatenate((x,np.ones((x.shape[0],1))),axis=1)\n",
    "        y = np.matmul(xstar,self.W)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "        return y\n",
    "\n",
    "    #backward pass\n",
    "    def backward(self,dLdy):\n",
    "        dydx = self.W\n",
    "        dydx = dydx[0:-1][:]\n",
    "        dydx = np.transpose(dydx)\n",
    "        dLdx = np.matmul(dLdy,dydx)\n",
    "        dldW = np.matmul(np.transpose(self.x),dLdy)\n",
    "        self.dldW = dldW\n",
    "        self.dldb = np.matmul(np.ones((1,self.x.shape[0])),dLdy)\n",
    "\n",
    "        return dLdx\n",
    "\n",
    "    def update_func(self,learning_rate):\n",
    "        self.W[:-1] = self.W[:-1] - learning_rate*self.dldW\n",
    "        self.W[-1] = self.W[-1] - learning_rate*self.dldb\n",
    "\n",
    "\n",
    "#RELU class is a class build for non-linearity for both forward and backward passing\n",
    "\n",
    "class relu_layer():\n",
    "    #Initialization\n",
    "    def __init__(self,num_relu_nodes):\n",
    "        self.num_relu_nodes = num_relu_nodes\n",
    "        self.x = collections.defaultdict(float)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x_array = np.array(x)\n",
    "        self.x = x_array\n",
    "        compare_matrix = np.zeros(np.shape(x_array))\n",
    "        y = np.maximum(x_array,compare_matrix)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def backward(self,dLdy):\n",
    "        dydx = self.x\n",
    "        dydx[dydx>0] = 1\n",
    "        dydx[dydx<0] = 0\n",
    "        #only propagate those which are selected\n",
    "        dLdx = np.multiply(dLdy,dydx)\n",
    "\n",
    "        return dLdx\n",
    "\n",
    "#class for back propagation\n",
    "class soft_max_cross_entropy_layer():\n",
    "    def __init__(self,num_input,num_class):\n",
    "\n",
    "        self.input = collections.defaultdict(float)\n",
    "        self.target = np.zeros((1,num_class))\n",
    "        self.output = np.zeros((1,num_class))\n",
    "        self.loss = collections.defaultdict(float)\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self,input,target):\n",
    "        self.input = np.array(input)\n",
    "        self.target = np.array(target)\n",
    "        dim_max = np.reshape(np.amax(self.input,axis=1),[-1,1])\n",
    "        input_adjusted = np.subtract(self.input,np.matlib.repmat(dim_max,1,self.input.shape[1]))\n",
    "        self.output = self.softmax(input_adjusted)\n",
    "        log_output = np.log(self.output+1e-10)\n",
    "        self.loss = -np.mean(np.sum(np.multiply(self.target,log_output),axis=1),axis=0)\n",
    "\n",
    "        return self.loss, self.output\n",
    "\n",
    "    #backward passing\n",
    "    def backward(self):\n",
    "        dLdx = self.output - self.target\n",
    "\n",
    "        return dLdx\n",
    "\n",
    "    #softmax\n",
    "    def softmax(self,input):\n",
    "        np_input = np.array(input)\n",
    "        num_dim = input.shape[1]\n",
    "        exp_input = np.exp(np_input)\n",
    "        norm_factor = np.matlib.repmat(np.reshape(np.sum(exp_input,axis=1),[-1,1]),1,num_dim)\n",
    "        soft_max_output = np.divide(exp_input,norm_factor)\n",
    "\n",
    "        return soft_max_output\n",
    "\n",
    "#a function to calculate accyracy\n",
    "def accuracy(y_pred, y_train):\n",
    "    a = y_pred\n",
    "    b = y_train\n",
    "    return sum(np.equal(np.argmax(a, axis=1), np.argmax(b, axis=1)))/b.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "class conv_layer_2d():\n",
    "    # define the initialization function\n",
    "    def __init__(self,num_kernel_size,num_stride,input_channel,output_channel,padding_mode):\n",
    "\n",
    "        self.kernel_height = num_kernel_size[0]\n",
    "        self.kernel_width = num_kernel_size[1]\n",
    "        self.height_stride = num_stride[0]\n",
    "        self.width_stride = num_stride[1]\n",
    "        self.padding_mode = padding_mode\n",
    "        self.input_height = 0\n",
    "        self.input_width = 0\n",
    "        self.input_height_padded = 0\n",
    "        self.input_width_padded = 0\n",
    "        self.input_channel = input_channel\n",
    "        self.output_height = 0\n",
    "        self.output_width = 0\n",
    "        self.output_channel = output_channel\n",
    "        # initialize weights\n",
    "        self.weight = np.random.normal(size=[self.input_channel,self.kernel_height,self.kernel_width,self.output_channel])\n",
    "        # for gradient variables\n",
    "        self.dydw = 0\n",
    "        self.dldw = 0\n",
    "        self.dldx = 0\n",
    "        # padding legnth\n",
    "        self.pad_top = 0\n",
    "        self.pad_bottom = 0\n",
    "        self.pad_left = 0\n",
    "        self.pad_right = 0\n",
    "\n",
    "    def forward(self,data):\n",
    "\n",
    "        assert len(data.shape) == 4\n",
    "        num_data = data.shape[0]\n",
    "        self.input_height = data.shape[1]\n",
    "        self.input_width = data.shape[2]\n",
    "        self.input_channel = data.shape[3]\n",
    "\n",
    "        if self.padding_mode == 'SAME':     # similar to tensorflow, here we allow different padding to different dimensions\n",
    "            self.output_height = math.ceil((float)(self.input_height)/(float)(self.height_stride))\n",
    "            self.output_width = math.ceil((float)(self.input_width)/(float)(self.width_stride))\n",
    "        elif self.padding_mode == 'VALID':     # no padding for the 'VALID' option\n",
    "            self.output_height = math.ceil((float)(self.input_height-self.kernel_height+1)/(float)(self.height_stride))\n",
    "            self.output_width = math.ceil((float)(self.input_width-self.kernel_width+1)/(float)(self.width_stride))\n",
    "        else:\n",
    "            raise ValueError('The padding method cannot be recognized! Please check your codes!')\n",
    "\n",
    "            \n",
    "        if(self.input_height%self.height_stride==0):\n",
    "            pad_height = max(self.kernel_height-self.height_stride,0)\n",
    "        else:\n",
    "            pad_height = max(self.kernel_height - (self.input_height%self.height_stride), 0)\n",
    "\n",
    "        if(self.input_width%self.width_stride==0):\n",
    "            pad_width = max(self.kernel_width-self.width_stride,0)\n",
    "        else:\n",
    "            pad_width = max(self.kernel_width - (self.input_width%self.width_stride), 0)\n",
    "\n",
    "\n",
    "        self.pad_top = pad_height//2\n",
    "        self.pad_bottom = pad_height - self.pad_top\n",
    "\n",
    "        self.pad_left = pad_width//2\n",
    "        self.pad_right = pad_width - self.pad_left\n",
    "\n",
    "        if self.padding_mode == 'SAME':\n",
    "            data_padded = np.pad(data,((0,0),(self.pad_top,self.pad_bottom),(self.pad_left,self.pad_right),(0,0)),'constant',constant_values=0)\n",
    "        else:\n",
    "            data_padded = np.pad(data,((0,0),(0,0),(0,0),(0,0)),'constant',constant_values=0)   # to keep the formation the same\n",
    "        self.input_height_padded = data_padded.shape[1]\n",
    "        self.input_width_padded = data_padded.shape[2]\n",
    "        \n",
    "        prod_tensor_inverse = np.zeros(shape=[self.output_channel,self.output_height,self.output_width, \n",
    "                                    self.input_height_padded,self.input_width_padded,self.input_channel])\n",
    "        \n",
    "        dydw_tensor_invere = np.zeros(shape=[self.output_height,self.output_width, num_data,\n",
    "                                    self.kernel_height,self.kernel_width,self.input_channel])\n",
    "        \n",
    "        weight_trans = np.transpose(self.weight,(3,1,2,0))\n",
    "        \n",
    "        for conv_channel in range(self.output_channel):\n",
    "            for conv_row_out in range(self.output_height):\n",
    "                for conv_column_out in range(self.output_width):\n",
    "                    this_start_row = conv_row_out*self.height_stride\n",
    "                    this_end_row = this_start_row + self.kernel_height\n",
    "                    this_start_col = conv_column_out*self.width_stride\n",
    "                    this_end_col = this_start_col + self.kernel_width\n",
    "                    prod_tensor_inverse[conv_channel,conv_row_out,conv_column_out,this_start_row:this_end_row,this_start_col:this_end_col] = weight_trans[conv_channel]\n",
    "                    if conv_channel == 0:\n",
    "                        dydw_tensor_invere[conv_row_out,conv_column_out] = data_padded[:,this_start_row:this_end_row,this_start_col:this_end_col,:]\n",
    "\n",
    "        self.dydw = np.transpose(dydw_tensor_invere,(2,1,0,3,4,5))\n",
    "        \n",
    "        self.dydx = np.transpose(prod_tensor_inverse,(0,2,1,3,4,5))\n",
    "        \n",
    "        prod_tensor = np.transpose(prod_tensor_inverse,(5,4,3,1,2,0))\n",
    "\n",
    "        conv_data = np.tensordot(data_padded,prod_tensor,axes=([3,2,1],[0,1,2]))\n",
    "\n",
    "        return conv_data\n",
    "\n",
    "    def backward(self,dldy):\n",
    "        num_data = dldy.shape[0]\n",
    "\n",
    "        dldy_inv_weight = np.transpose(dldy,(3,1,2,0))        \n",
    "        \n",
    "        dldw_inv  = np.tensordot(dldy_inv_weight,self.dydw,axes=([3,2,1],[0,1,2]))\n",
    "        self.dldw = np.transpose(dldw_inv,(3,1,2,0))\n",
    "\n",
    "        dldx_padded = np.tensordot(dldy,self.dydx,axes=([3,2,1],[0,1,2]))  \n",
    "\n",
    "        self.dldx = dldx_padded[:,self.pad_top:self.pad_top+self.input_height,self.pad_left:self.pad_left+self.input_width,:]\n",
    "\n",
    "        return self.dldx\n",
    "\n",
    "    def update_func(self,learning_rate=1e-3):\n",
    "        self.weight = self.weight - learning_rate*self.dldw\n",
    "\n",
    "# class for max-pooling\n",
    "class max_pooling_2d():\n",
    "\n",
    "    def __init__(self,num_kernel_size,num_stride,padding_mode):\n",
    "        self.kernel_height = num_kernel_size[0]\n",
    "        self.kernel_width = num_kernel_size[1]\n",
    "        self.height_stride = num_stride[0]\n",
    "        self.width_stride = num_stride[1]\n",
    "        self.padding_mode = padding_mode\n",
    "        self.input_height = 0\n",
    "        self.input_width = 0\n",
    "        self.input_height_padded = 0\n",
    "        self.input_width_padded = 0\n",
    "        self.num_channel = 0\n",
    "\n",
    "        self.dydx = 0\n",
    "        self.dldx = 0\n",
    "\n",
    "        self.pad_top = 0\n",
    "        self.pad_bottom = 0\n",
    "        self.pad_left = 0\n",
    "        self.pad_right = 0\n",
    "\n",
    "    def forward(self,data):\n",
    "\n",
    "        num_data = data.shape[0]\n",
    "        self.input_height = data.shape[1]\n",
    "        self.input_width = data.shape[2]\n",
    "        self.num_channel = data.shape[3]\n",
    "\n",
    "        if self.padding_mode == 'SAME':     \n",
    "            self.output_height = math.ceil((float)(self.input_height)/(float)(self.height_stride))\n",
    "            self.output_width = math.ceil((float)(self.input_width)/(float)(self.width_stride))\n",
    "        elif self.padding_mode == 'VALID':\n",
    "            self.output_height = math.ceil((float)(self.input_height-self.kernel_height+1)/(float)(self.height_stride))\n",
    "            self.output_width = math.ceil((float)(self.input_width-self.kernel_width+1)/(float)(self.width_stride))\n",
    "\n",
    "\n",
    "        if(self.input_height%self.height_stride==0):\n",
    "            pad_height = max(self.kernel_height-self.height_stride,0)\n",
    "        else:\n",
    "            pad_height = max(self.kernel_height - (self.input_height%self.height_stride), 0)\n",
    "\n",
    "        if(self.input_width%self.width_stride==0):\n",
    "            pad_width = max(self.kernel_width-self.width_stride,0)\n",
    "        else:\n",
    "            pad_width = max(self.kernel_width - (self.input_width%self.width_stride), 0)\n",
    "\n",
    "        self.pad_top = pad_height//2\n",
    "        self.pad_bottom = pad_height - self.pad_top\n",
    "        \n",
    "        self.pad_left = pad_width//2\n",
    "        self.pad_right = pad_width - self.pad_left\n",
    "        \n",
    "        if self.padding_mode == 'SAME':\n",
    "            data_padded = np.pad(data,((0,0),(self.pad_top,self.pad_bottom),(self.pad_left,self.pad_right),(0,0)),mode='constant',constant_values=0)\n",
    "        else:\n",
    "            data_padded = np.pad(data,((0,0),(0,0),(0,0),(0,0)),mode='constant',constant_values=0)\n",
    "        self.input_height_padded = data_padded.shape[1]\n",
    "        self.input_width_padded = data_padded.shape[2]\n",
    "\n",
    "        inv_pooled_tensor = np.zeros(shape=[self.output_height,self.output_width,num_data,self.num_channel])\n",
    "        \n",
    "        dydx_mask_inv_tensor = np.zeros(shape=[self.output_height,self.output_width,num_data,self.num_channel,self.input_height_padded,self.input_width_padded])\n",
    "\n",
    "        for cRow in range(self.output_height):\n",
    "            for cCol in range(self.output_width):\n",
    "                this_start_row = cRow*self.height_stride\n",
    "                this_end_row = this_start_row + self.kernel_height\n",
    "                this_start_col = cCol*self.width_stride\n",
    "                this_end_col = this_start_col + self.kernel_width\n",
    "\n",
    "                this_image_crop = data_padded[:,this_start_row:this_end_row,this_start_col:this_end_col,:]    # [num_data * kernel_height * kernel_width * num_channel]\n",
    "                this_image_crop_inv = np.transpose(this_image_crop,(0,2,3,1))                                 # [num_data * num_channel * kernel_height * kernel_width]\n",
    "                this_image_crop_inv_flatten = np.reshape(this_image_crop_inv,[num_data,self.num_channel,-1])        # [num_data * num_channel * (kernel_height * kernel_width)]\n",
    "                inv_pooled_tensor[cRow,cCol] = np.amax(this_image_crop_inv_flatten,axis=2)        # [num_data * num_channel]\n",
    "\n",
    "                ind_max_pixel = np.reshape(np.argmax(this_image_crop_inv_flatten,axis=2),[num_data,self.num_channel,1])\n",
    "\n",
    "                this_one_hot_mask = np.zeros(shape=this_image_crop_inv_flatten.shape)\n",
    "                this_one_hot_mask[:,:,ind_max_pixel] = 1\n",
    "                this_one_hot_mask = np.reshape(this_one_hot_mask,[num_data,self.num_channel,self.kernel_height,self.kernel_width])   # [num_data * num_channel * kernel_height * kernel_width]\n",
    "\n",
    "                dydx_mask_inv_tensor[cRow,cCol,:,:,this_start_row:this_end_row,this_start_col:this_end_col] = this_one_hot_mask[:]\n",
    "        self.dydx = np.transpose(dydx_mask_inv_tensor,(2,0,1,4,5,3))\n",
    "        pooling_output = np.transpose(inv_pooled_tensor,(2,0,1,3))\n",
    "\n",
    "        return pooling_output\n",
    "\n",
    "    def backward(self,dldy):\n",
    "        assert len(dldy.shape)==4\n",
    "        num_data = dldy.shape[0]\n",
    "\n",
    "        dldy_compute_inv = np.zeros(shape=[self.input_height_padded,self.input_width_padded,num_data,self.output_height,self.output_width,self.num_channel])\n",
    "        dldy_compute_inv[:,:] = dldy\n",
    "        dldy_compute = np.transpose(dldy_compute_inv,(2,3,4,0,1,5))     # [num_data * nHeight_out * nWidth_out * nHeight_padded * nWidth_padded * num_channel]\n",
    "\n",
    "        dldx_tensor = np.einsum('...,...->...',dldy_compute,self.dydx)\n",
    "\n",
    "        self.dydx = np.sum(np.sum(dldx_tensor,axis=1),axis=1)\n",
    "\n",
    "        return self.dydx\n",
    "\n",
    "#calculate training errors\n",
    "def get_error_loss_and_pred(inter_data, label):\n",
    "\n",
    "    for conv_layer in range(len(conv_layer_list)):\n",
    "        inter_data = conv_layer_list[conv_layer].forward(inter_data)\n",
    "        if (conv_layer+1) in pool_operationum_layers:\n",
    "                this_pooling_ind = pool_operationum_layers.index(conv_layer+1)\n",
    "                inter_data = pool_layer_list[this_pooling_ind].forward(inter_data)\n",
    "    \n",
    "    batch_image_out_shape = inter_data.shape\n",
    "    \n",
    "    inter_data = np.reshape(inter_data,[-1,flattenum_size])\n",
    "    inter_data = linear_layer_1.forward(inter_data)\n",
    "    inter_data = nonum_linear_layer_1.forward(inter_data)\n",
    "    z_out = linear_layer_2.forward(inter_data)\n",
    "    \n",
    "    \n",
    "    loss, softmax_predict = loss_layer.forward(z_out,label)\n",
    "    error = 1 - accuracy(softmax_predict, label)\n",
    "    return error, loss, softmax_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../Mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../Mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../Mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../Mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../Mnist'\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "\n",
    "train_image = mnist.train.images\n",
    "train_label = mnist.train.labels\n",
    "test_image = mnist.test.images\n",
    "test_label = mnist.test.labels\n",
    "\n",
    "train_image_feed = np.reshape(train_image,[-1,28,28,1])\n",
    "test_image_feed = np.reshape(test_image,[-1,28,28,1])\n",
    "\n",
    "num_data_train = train_image_feed.shape[0]\n",
    "num_data_test = test_image_feed.shape[0]\n",
    "\n",
    "\n",
    "conv_kernel_size_list = [(3,3),(3,3)]\n",
    "# conv_stride_size_list = [(1,1),(1,1)]\n",
    "conv_stride_size_list = [(2,2),(2,2)]\n",
    "# pool_operationum_layers = [1,2]\n",
    "pool_operationum_layers = []\n",
    "\n",
    "pooling_kernel_size_list = [(2,2),(2,2)]\n",
    "pooling_stride_size_list = [(2,2),(2,2)]\n",
    "image_channels = [1,3,9]\n",
    "flattenum_size = 7*7*image_channels[-1]\n",
    "num_linear_node = 100\n",
    "num_class = 10\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "cnnum_learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_layer_list = []\n",
    "pool_layer_list = []\n",
    "\n",
    "conv_layer_1 = conv_layer_2d(num_kernel_size=conv_kernel_size_list[0],\n",
    "                                      num_stride=conv_stride_size_list[0],\n",
    "                                      input_channel=image_channels[0],\n",
    "                                      output_channel=image_channels[1],\n",
    "                                      padding_mode='SAME')\n",
    "conv_layer_list.append(conv_layer_1)\n",
    "\n",
    "pool_layer_1 = max_pooling_2d(num_kernel_size=pooling_kernel_size_list[0],\n",
    "                              num_stride=pooling_stride_size_list[0],\n",
    "                              padding_mode='SAME')\n",
    "\n",
    "pool_layer_list.append(pool_layer_1)\n",
    "\n",
    "conv_layer_2 = conv_layer_2d(num_kernel_size=conv_kernel_size_list[1],\n",
    "                                      num_stride=conv_stride_size_list[1],\n",
    "                                      input_channel=image_channels[1],\n",
    "                                      output_channel=image_channels[2],\n",
    "                                      padding_mode='SAME')\n",
    "conv_layer_list.append(conv_layer_2)\n",
    "\n",
    "pool_layer_2 = max_pooling_2d(num_kernel_size=pooling_kernel_size_list[1],\n",
    "                              num_stride=pooling_stride_size_list[1],\n",
    "                              padding_mode='SAME')\n",
    "\n",
    "pool_layer_list.append(pool_layer_2)\n",
    "\n",
    "linear_layer_1 = linear_layer(flattenum_size,num_linear_node)\n",
    "nonum_linear_layer_1 = relu_layer(num_linear_node)\n",
    "linear_layer_2 = linear_layer(num_linear_node,num_class)\n",
    "loss_layer = soft_max_cross_entropy_layer(num_class,num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5, train error = 0.234236, test error = 0.225600, train_loss = 0.874433, test loss = 0.820152, time = 14.50\n",
      "1/5, train error = 0.173509, test error = 0.162800, train_loss = 0.587688, test loss = 0.560668, time = 14.05\n",
      "2/5, train error = 0.137218, test error = 0.133400, train_loss = 0.487266, test loss = 0.467245, time = 14.32\n",
      "3/5, train error = 0.122418, test error = 0.114800, train_loss = 0.436601, test loss = 0.414732, time = 14.80\n",
      "4/5, train error = 0.108327, test error = 0.106400, train_loss = 0.382182, test loss = 0.374319, time = 14.66\n",
      "model saved to ../models/NP_CNN.obj\n",
      "data saved to ../models/\n"
     ]
    }
   ],
   "source": [
    "train_error_list = []\n",
    "train_loss_list = []\n",
    "test_error_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "hm_epochs = 100\n",
    "for epoch in range(hm_epochs):\n",
    "    t_ref = time.time()\n",
    "    current_epoch_loss = 0\n",
    "    train_ind_rand = np.random.choice(num_data_train,size=num_data_train)\n",
    "    for i in range(1000):\n",
    "        print(\"\\r{}, {}\".format(i,1000), end=\"\")\n",
    "\n",
    "        this_train_ind = train_ind_rand[i*batch_size:(i+1)*batch_size]\n",
    "        this_train_image = train_image_feed[this_train_ind]\n",
    "        this_train_label = train_label[this_train_ind]\n",
    "        inter_data = this_train_image[:]\n",
    "        \n",
    "        for conv_layer in range(len(conv_layer_list)):\n",
    "            inter_data = conv_layer_list[conv_layer].forward(inter_data)\n",
    "            if (conv_layer+1) in pool_operationum_layers:\n",
    "                this_pooling_ind = pool_operationum_layers.index(conv_layer+1)\n",
    "                inter_data = pool_layer_list[this_pooling_ind].forward(inter_data)\n",
    "\n",
    "        batch_image_out_shape = inter_data.shape\n",
    "\n",
    "        inter_data = np.reshape(inter_data,[-1,flattenum_size])\n",
    "        inter_data = linear_layer_1.forward(inter_data)\n",
    "        inter_data = nonum_linear_layer_1.forward(inter_data)\n",
    "        z_out = linear_layer_2.forward(inter_data)\n",
    "\n",
    "        this_loss, _ = loss_layer.forward(z_out,this_train_label)\n",
    "        current_epoch_loss += this_loss\n",
    "\n",
    "        \n",
    "        dldy = loss_layer.backward()\n",
    "\n",
    "        \n",
    "        dldy = linear_layer_2.backward(dldy)\n",
    "        linear_layer_2.update_func(learning_rate=learning_rate)\n",
    "        dldy = nonum_linear_layer_1.backward(dldy)\n",
    "\n",
    "        dldy = linear_layer_1.backward(dldy)\n",
    "        linear_layer_1.update_func(learning_rate=learning_rate)\n",
    "\n",
    "        dldy = np.reshape(dldy,batch_image_out_shape)\n",
    "        for conv_layer in range(len(conv_layer_list)):\n",
    "\n",
    "            this_layer_number = len(conv_layer_list) - conv_layer\n",
    "            if this_layer_number in pool_operationum_layers:\n",
    "                this_pooling_ind = pool_operationum_layers.index(this_layer_number)\n",
    "                dldy = pool_layer_list[this_pooling_ind].backward(dldy)\n",
    "\n",
    "            conv_input = -(conv_layer + 1)\n",
    "            dldy = conv_layer_list[conv_input].backward(dldy)\n",
    "            conv_layer_list[conv_input].update_func(learning_rate=cnnum_learning_rate)\n",
    "\n",
    "    train_error, train_loss, y_train_pred = get_error_loss_and_pred(train_image_feed, train_label)\n",
    "    test_error, test_loss, y_test_pred = get_error_loss_and_pred(test_image_feed, test_label)\n",
    "    \n",
    "    train_error_list.append(train_error)\n",
    "    test_error_list.append(test_error)\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    print(\"\\r{}/{}, train error = {:.6f}, test error = {:.6f}, train_loss = {:.6f}, test loss = {:.6f}, time = {:.2f}\".format(epoch, hm_epochs, train_error, test_error, train_loss, test_loss, time.time() - t_ref))    \n",
    "class_dict = {}\n",
    "class_dict[\"conv_layer_list\"] = conv_layer_list\n",
    "class_dict[\"pool_layer_list\"] = pool_layer_list\n",
    "class_dict[\"linear_layer_1\"] = linear_layer_1\n",
    "class_dict[\"nonum_linear_layer_1\"] = nonum_linear_layer_1\n",
    "class_dict[\"linear_layer_2\"] = linear_layer_2\n",
    "class_dict[\"loss_layer\"] = loss_layer\n",
    "\n",
    "save_model(class_dict)\n",
    "save_learning_curve(train_error_list, test_error_list, train_loss_list, test_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recover model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class_dict = load_model()\n",
    "# conv_layer_list = class_dict[\"conv_layer_list\"]\n",
    "# pool_layer_list = class_dict[\"pool_layer_list\"]\n",
    "# linear_layer_1 = class_dict[\"linear_layer_1\"]\n",
    "# nonum_linear_layer_1 = class_dict[\"nonum_linear_layer_1\"]\n",
    "# linear_layer_2 = class_dict[\"linear_layer_2\"]\n",
    "# loss_layer = class_dict[\"loss_layer\"]\n",
    "\n",
    "# train_error, train_loss = get_error_and_loss(train_image_feed)\n",
    "# test_error, test_loss = get_error_and_loss(test_image_feed)\n",
    "\n",
    "# print(\"train error = {:.6f}, test error = {:.6f},. train_loss = {:.6f}, test loss = {:.6f}\".format(train_error, test_error, train_loss, test_loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 * 15)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
