{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this notebook is ready to run, default to test the recover models for three games for 100 test and get the mean performance in terms of mean of score and discounted reward, if you want to render, just change the render flag below to be true, then each game will render once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If this is True, each of the three games will play once and only once with render\n",
    "# If this False, each of the three games will play 100 times without render, \n",
    "# to calculate the mean of score and discounted returns\n",
    "render_flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recover models for games sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MsPacgirl\n",
      "\n",
      "../models/B_4_3/MsPacgirl/B_4_3-MsPacgirl-*36.51*.checkpoint\n",
      "INFO:tensorflow:Restoring parameters from ../models/B_4_3/MsPacgirl/B_4_3-MsPacgirl-*36.51*.checkpoint\n",
      "\n",
      "checking performace\n",
      "Finished in 00:01:47                                                                                            \n",
      "MsPacgirl, score mean = 35.45, dis return mean = 4.716168055875034\n",
      "Boxing\n",
      "\n",
      "../models/B_4_3/Boxing/B_4_3-Boxing-*4.2*.checkpoint\n",
      "INFO:tensorflow:Restoring parameters from ../models/B_4_3/Boxing/B_4_3-Boxing-*4.2*.checkpoint\n",
      "\n",
      "checking performace\n",
      "Finished in 00:05:37                                                                                            \n",
      "Boxing, score mean = 4.69, dis return mean = -0.39701310973007975\n",
      "Pong\n",
      "\n",
      "../models/B_4_3/Pong/B_4_3-Pong-*-18.6*.checkpoint\n",
      "INFO:tensorflow:Restoring parameters from ../models/B_4_3/Pong/B_4_3-Pong-*-18.6*.checkpoint\n",
      "\n",
      "checking performace\n",
      "Finished in 00:03:12                                                                                            \n",
      "Pong, score mean = -18.62, dis return mean = -0.6959015027976722\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys,time\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# a function to estimate the time remaining, home-made, skip reading it\n",
    "class time_est():\n",
    "    def __init__(self, total_len):\n",
    "        self.t_start = time.time()\n",
    "        self.total_len = total_len\n",
    "        self.count = 0\n",
    "        self.t_ref = time.time()\n",
    "    \n",
    "    def check(self,no_of_check=1,info=\"\"):\n",
    "        self.count += no_of_check\n",
    "        if time.time() - self.t_ref > 1 and self.count > 0:\n",
    "            t_used = time.time() - self.t_start\n",
    "            t_total = t_used * self.total_len / self.count\n",
    "            t_remain = t_total - t_used\n",
    "            process_bar = \"|\"\n",
    "            for i in range(40):\n",
    "                if (i/40) < (self.count/self.total_len):\n",
    "                    process_bar += \"█\"\n",
    "                else:\n",
    "                    process_bar += \" \"\n",
    "            process_bar += \"|\"\n",
    "            if info != \"\":\n",
    "                info = str(info) + \"  \"\n",
    "            print(\"\\r\" + (str(info) + \"{:.2f}% ({}/{})  \".format(self.count * 100/self.total_len, self.count,self.total_len)) \n",
    "                  + str(process_bar).ljust(45) \n",
    "                  + \"Used: {:02.0f}:{:02.0f}:{:02.0f}\".format(int(t_used/3600), int(t_used/60)%60, t_used % 60).ljust(16) \n",
    "                  + \"ETA: {:02.0f}:{:02.0f}:{:02.0f}\".format(int(t_remain/3600), int(t_remain/60)%60, t_remain % 60),end=\"\")\n",
    "            self.t_ref = time.time()\n",
    "        if self.count == self.total_len:\n",
    "            t_used = time.time() - self.t_start\n",
    "            if info != \"\":\n",
    "                info = str(info) + \"  \"\n",
    "            print(\"\\r\" + str(info) + \"Finished in \" \n",
    "                  + \"{:02.0f}:{:02.0f}:{:02.0f}\".format(int(t_used/3600), int(t_used/60)%60, t_used % 60).ljust(100))\n",
    "    def get(self,no_of_check=1):\n",
    "        process_bar = \"|\"\n",
    "        for i in range(40):\n",
    "            if (i/40) < (self.count/self.total_len):\n",
    "                process_bar += \"█\"\n",
    "            else:\n",
    "                process_bar += \" \"\n",
    "        process_bar += \"|\"\n",
    "        self.count += no_of_check\n",
    "        t_used = time.time() - self.t_start\n",
    "        t_total = t_used * self.total_len / self.count\n",
    "        t_remain = t_total - t_used\n",
    "        return \"{} ETA: {:02.0f}:{:02.0f}:{:02.0f}\".format(process_bar, int(t_remain/3600), int(t_remain/60)%60, t_remain % 60)\n",
    "# disable tensorflow debugging information\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def testing(game_name, render_flag):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # print the game_name\n",
    "    print(game_name)\n",
    "    file_name = \"B_4_3\"\n",
    "    path = \"../models/{}/{}\".format(file_name,game_name)\n",
    "\n",
    "\n",
    "    optimizer = \"RMS\"\n",
    "\n",
    "    # setup hyperparameter with respect to different games\n",
    "    if game_name == 'Pong':\n",
    "        env = gym.make(\"Pong-v4\")\n",
    "        env_1 = gym.make(\"Pong-v4\")\n",
    "        num_of_actions = 6\n",
    "        max_runs = 2000000\n",
    "        Learning_rate = 1e-4 # 1e-7\n",
    "        final_greedy_rate = 0.1\n",
    "        discount = .99\n",
    "    elif game_name == 'MsPacgirl':\n",
    "        env = gym.make(\"MsPacman-v4\")\n",
    "        env_1 = gym.make(\"MsPacman-v4\")\n",
    "        num_of_actions = 9\n",
    "        max_runs = 2000000\n",
    "        Learning_rate = 1e-4 # 1e-7\n",
    "        final_greedy_rate = 0.1\n",
    "        discount = .99\n",
    "    elif game_name == 'Boxing':\n",
    "        env = gym.make(\"Boxing-v4\")\n",
    "        env_1 = gym.make(\"Boxing-v4\")\n",
    "        num_of_actions = 18\n",
    "        max_runs = 2000000\n",
    "        Learning_rate = 1e-4 # 1e-7\n",
    "        final_greedy_rate = 0.1\n",
    "        discount = .99\n",
    "    else:\n",
    "        raise ValueError('Unidentified game mode')\n",
    "\n",
    "\n",
    "\n",
    "    # a function to convert image from rgd to grey, since color is not useful, a grey image can simplify\n",
    "    def rgb2gray(rgb):\n",
    "        r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "        gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "        return gray\n",
    "\n",
    "\n",
    "    # some hyperparameters\n",
    "    batch_size = 20\n",
    "    #hyperparameters for CNN\n",
    "    width = 6\n",
    "    height = 6\n",
    "    layer_1_unit = 16\n",
    "    layer_2_unit = 32\n",
    "\n",
    "    #size of the image\n",
    "    image_size = 28\n",
    "    \n",
    "    # define tensorflow place holders\n",
    "    reward_place = tf.placeholder(tf.float32, [None])\n",
    "    obs_new = tf.placeholder(tf.float32, [None,image_size,image_size,4])\n",
    "    obs = tf.placeholder(tf.float32, [None,image_size,image_size,4])\n",
    "    act = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "\n",
    "    # two dimensional convolutional\n",
    "    def convolution2D(DataIn,Weight):\n",
    "        return tf.nn.conv2d(DataIn,Weight,strides = [1,2,2,1], padding = 'SAME')         \n",
    "\n",
    "    # a function to define the neural network model\n",
    "    def define_model(seed_num):\n",
    "        tf.set_random_seed(seed_num)\n",
    "        W1 = tf.Variable(tf.random_normal([width, height, 4, layer_1_unit]))\n",
    "        b1 = tf.Variable(tf.random_normal([layer_1_unit]))\n",
    "\n",
    "        y1 = tf.nn.relu(convolution2D(obs,W1)+b1)\n",
    "        y1_new = tf.nn.relu(convolution2D(obs_new,W1)+b1)\n",
    "\n",
    "        W2 = tf.Variable(tf.random_normal([width, height, layer_1_unit, layer_2_unit]))\n",
    "        b2 = tf.Variable(tf.random_normal([layer_2_unit]))\n",
    "\n",
    "        y2 = tf.nn.relu(convolution2D(y1,W2)+b2)\n",
    "        y2_new = tf.nn.relu(convolution2D(y1_new,W2)+b2)\n",
    "\n",
    "        # flatten the tensor before put into the fully connected layer\n",
    "        flatten_shape = int(image_size/4)*int(image_size/4)*layer_2_unit\n",
    "        W3 = tf.Variable(tf.random_normal([flatten_shape,256]))\n",
    "        b3 = tf.Variable(tf.random_normal([256]))\n",
    "\n",
    "        y3 = tf.nn.relu(tf.matmul(tf.reshape(y2,[-1,flatten_shape]),W3)+b3)\n",
    "        y3_new = tf.nn.relu(tf.matmul(tf.reshape(y2_new,[-1,flatten_shape]),W3)+b3)\n",
    "\n",
    "        W4 = tf.Variable(tf.random_normal([256,num_of_actions]))\n",
    "        b4 = tf.Variable(tf.random_normal([num_of_actions]))\n",
    "\n",
    "        y4 = tf.matmul(y3,W4) + b4\n",
    "        y4_new = tf.matmul(y3_new,W4) + b4\n",
    "\n",
    "        # target network which has the same structure, to copy the previous network\n",
    "        W1_target = tf.Variable(tf.random_normal([width, height, 4, layer_1_unit]))\n",
    "        b1_target = tf.Variable(tf.random_normal([layer_1_unit]))\n",
    "\n",
    "        y1_target = tf.nn.relu(convolution2D(obs, W1_target) + b1_target)\n",
    "        y1_new_target = tf.nn.relu(convolution2D(obs_new, W1_target) + b1_target)\n",
    "\n",
    "        W2_target = tf.Variable(\n",
    "            tf.random_normal([width, height, layer_1_unit, layer_2_unit]))\n",
    "        b2_target = tf.Variable(tf.random_normal([layer_2_unit]))\n",
    "\n",
    "        y2_target = tf.nn.relu(convolution2D(y1_target, W2_target) + b2_target)\n",
    "        y2_new_target = tf.nn.relu(convolution2D(y1_new_target, W2_target) + b2_target)\n",
    "\n",
    "        flatten_shape = int(image_size/4)*int(image_size/4)*layer_2_unit\n",
    "        W3_target = tf.Variable(tf.random_normal([flatten_shape, 256]))\n",
    "        b3_target = tf.Variable(tf.random_normal([256]))\n",
    "\n",
    "        y3_target = tf.nn.relu(tf.matmul(tf.reshape(y2_target, [-1, flatten_shape]), W3_target) + b3_target)\n",
    "        y3_new_target = tf.nn.relu(tf.matmul(tf.reshape(y2_new_target, [-1, flatten_shape]), W3_target) + b3_target)\n",
    "\n",
    "        W4_target = tf.Variable(tf.random_normal([256, num_of_actions]))\n",
    "        b4_target = tf.Variable(tf.random_normal([num_of_actions]))\n",
    "\n",
    "        y4_target = tf.matmul(y3_target, W4_target) + b4_target\n",
    "        y4_new_target = tf.matmul(y3_new_target, W4_target) + b4_target\n",
    "\n",
    "        # global same variables for updating the target network\n",
    "        global update_1, update_2, update_3, update_4, update_5, update_6, update_7, update_8\n",
    "\n",
    "        # variables to update the network\n",
    "        update_1 = W1_target.assign(W1)\n",
    "        update_2 = b1_target.assign(b1)\n",
    "        update_3 = W2_target.assign(W2)\n",
    "        update_4 = b2_target.assign(b2)\n",
    "        update_5 = W3_target.assign(W3)\n",
    "        update_6 = b3_target.assign(b3)\n",
    "        update_7 = W4_target.assign(W4)\n",
    "        update_8 = b4_target.assign(b4)\n",
    "\n",
    "        return y4,y4_new, y4_target, y4_new_target\n",
    "\n",
    "    # a function to update the target network\n",
    "    def update(sess):\n",
    "        sess.run(update_1)\n",
    "        sess.run(update_2)\n",
    "        sess.run(update_3)\n",
    "        sess.run(update_4)\n",
    "        sess.run(update_5)\n",
    "        sess.run(update_6)\n",
    "        sess.run(update_7)\n",
    "        sess.run(update_8)\n",
    "\n",
    "\n",
    "    def image_preprocess(image):\n",
    "        # if the image is Pong, cut the image so that the noisy part do not affect the network\n",
    "        if game_name == 'Pong':\n",
    "            image = image[34:194,:]\n",
    "        # process image to 28 by 28 grey image\n",
    "        img = Image.fromarray(image, 'RGB').convert('L')\n",
    "        img = img.resize((image_size,image_size),resample=Image.BILINEAR)\n",
    "        image_trans = np.asarray(img, dtype=np.uint8)\n",
    "        # binarize the image of pong to just two level for network to better understand the image\n",
    "        if game_name == 'Pong':\n",
    "            image_trans.setflags(write=True)\n",
    "            image_trans[image_trans>90] = 255\n",
    "            image_trans[image_trans<=90] = 0\n",
    "\n",
    "        return image_trans\n",
    "\n",
    "    # a function to run the game with the current model for 100 times and average the performance\n",
    "    def test_process(sess, action_max):\n",
    "        print(\"\\nchecking performace\")\n",
    "        if render_flag:\n",
    "            num_test = 1\n",
    "        else:\n",
    "            num_test = 100\n",
    "        est = time_est(num_test)\n",
    "        with sess.as_default():\n",
    "            # lists to store the the episode length and returns\n",
    "            score_list = []\n",
    "            dis_rtn_list = []\n",
    "            for i in range(num_test):\n",
    "                old_obs = env.reset()\n",
    "                t = 0\n",
    "                frame_buffer = deque(maxlen=4)\n",
    "                total_reward = 0\n",
    "                discount_factor = 1\n",
    "                discounted_value = 0\n",
    "                count = 0\n",
    "                observation_stack = []\n",
    "                while 1:\n",
    "                    if count!= 0:\n",
    "                        discount_factor *= 0.99\n",
    "                    # greedy policy\n",
    "                    if t > 3 and random.random() > final_greedy_rate:\n",
    "                        action_dic = {obs:[old_obs],obs_new:np.zeros([1,image_size,image_size,4]),act:[0],reward_place:[reward]}\n",
    "                        action = action_max.eval(action_dic)[0]\n",
    "                    else:\n",
    "                        action = round(random.uniform(0, num_of_actions - 1))\n",
    "                    if render_flag:\n",
    "                        env.render()\n",
    "                    # take action and get the response from the action\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    obs_frame = image_preprocess(observation)\n",
    "                    frame_buffer.append(obs_frame)\n",
    "\n",
    "                    if len(frame_buffer) == 4:\n",
    "                        observation_stack = np.stack(list(frame_buffer),axis=0)\n",
    "                        observation_stack = observation_stack.transpose([1,2,0])\n",
    "                    # reward either -1, 1 or 0\n",
    "                    reward = np.clip(reward,-1,1)\n",
    "                    total_reward += reward\n",
    "                    count += 1\n",
    "                    # save the observation to a variable\n",
    "                    old_obs = observation_stack\n",
    "                    t += 1\n",
    "                    # calculate the discounted returns\n",
    "                    discounted_value += discount_factor * reward\n",
    "\n",
    "                    if done:\n",
    "                        # saving the reward and discouned rewards into list\n",
    "                        score_list.append(total_reward)\n",
    "                        dis_rtn_list.append(discounted_value)\n",
    "                        break\n",
    "                est.check()\n",
    "        # return the mean of episode length and discounted rewards\n",
    "        return np.mean(score_list), np.mean(dis_rtn_list)\n",
    "\n",
    "\n",
    "    # function for training\n",
    "    def training(seed_num):\n",
    "        # define the greedy rate, initially 1, it can be changed in the training\n",
    "        greedy_rate = 1\n",
    "\n",
    "        # define the highes reward to be -21, it can be changed in the training\n",
    "        highest_reward = -21\n",
    "\n",
    "        # define some variables in tensorflow for training\n",
    "        y2, y2_new, y2_target, y2_target_new= define_model(seed_num)\n",
    "        # get action by Q values\n",
    "        action_max = tf.cast(tf.argmax(y2, axis=1), tf.int32)\n",
    "        # calculate bellman loss\n",
    "        data_amount = tf.shape(y2_new)[0]\n",
    "        Q_old_index = tf.concat([tf.reshape(tf.range(0,limit=data_amount),[data_amount,1]), tf.reshape(act,[data_amount,1])],axis=1)\n",
    "        old_q = tf.gather_nd(y2,Q_old_index)\n",
    "\n",
    "\n",
    "        action_max_primary = tf.cast(tf.argmax(y2_new, axis=1), tf.int32)\n",
    "        Q_new_index = tf.concat([tf.reshape(tf.range(0,limit=data_amount),[data_amount,1]), tf.reshape(action_max_primary,[data_amount,1])],axis=1)\n",
    "\n",
    "        max_q_value_next = tf.gather_nd(y2_target_new, Q_new_index)\n",
    "\n",
    "        change_in_q = (reward_place + discount * tf.stop_gradient(max_q_value_next) - old_q)\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(change_in_q)/2)\n",
    "\n",
    "        # optimizer can be changed by the setting\n",
    "        if optimizer == \"SGD\":\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate=Learning_rate).minimize(loss)\n",
    "        elif optimizer == \"ADAM\":\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=Learning_rate).minimize(loss)\n",
    "        elif optimizer == \"RMS\":\n",
    "            train_step = tf.train.RMSPropOptimizer(learning_rate=Learning_rate,decay=0.9,momentum=0.2,centered=True).minimize(loss)\n",
    "        else:\n",
    "            aaaaaaaaa        \n",
    "\n",
    "\n",
    "        # here comes the training code\n",
    "        with tf.Session() as sess:\n",
    "            print()\n",
    "            # name of the last checkpoint model\n",
    "            if game_name == \"Pong\":\n",
    "                model_name = \"{}/{}-{}-*{}*{}\".format(path,file_name,game_name,-18.6,'.checkpoint')\n",
    "            elif game_name == \"MsPacgirl\":\n",
    "                model_name = \"{}/{}-{}-*{}*{}\".format(path,file_name,game_name,36.51,'.checkpoint')\n",
    "            elif game_name == \"Boxing\":\n",
    "                model_name = \"{}/{}-{}-*{}*{}\".format(path,file_name,game_name,4.2,'.checkpoint')\n",
    "\n",
    "            print(model_name)\n",
    "            # recover model and some data if possible, again, not used, all model start from begining\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, model_name)\n",
    "            score_mean, dis_rtn_mean = test_process(sess, action_max)\n",
    "            print(\"{}, score mean = {}, dis return mean = {}\".format(game_name, score_mean, dis_rtn_mean))\n",
    "            \n",
    "    training(round(random.uniform(0,3000)))\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    \n",
    "    for game_name in [\"MsPacgirl\", \"Boxing\", \"Pong\"]:\n",
    "        testing(game_name, render_flag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
