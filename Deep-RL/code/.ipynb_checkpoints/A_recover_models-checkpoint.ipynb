{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this notebook is ready to run, default to test the recover models for each part for 100 test and get the mean performance in terms of mean of episode length and discounted returns, if you want to render, just change the render flag below to be true, then each part will render once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If this is True, each of the part will play once and only once with render\n",
    "# If this False, each of the part will play 100 times without render, \n",
    "# to calculate the mean of score and discounted returns\n",
    "render_flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A 5 30 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# disable tensorflow debugging information\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# make the gym enviorment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# define some fixed hyperparameters\n",
    "dis_factor = 0.99\n",
    "greedy_rate = 0.05\n",
    "# get the file name for storing\n",
    "file_name = \"test\"\n",
    "\n",
    "# define some place holder\n",
    "reward_place = tf.placeholder(tf.float32, [None])\n",
    "obs_new = tf.placeholder(tf.float32, [None,4])\n",
    "obs = tf.placeholder(tf.float32, [None,4])\n",
    "act = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "\n",
    "# define the model\n",
    "def define_model(seed_num, num_hidden_units):\n",
    "    tf.set_random_seed(seed_num)\n",
    "    W1 = tf.Variable(tf.random_normal([4, num_hidden_units]))\n",
    "    b1 = tf.Variable(tf.random_normal([num_hidden_units]))\n",
    "\n",
    "    y1 = tf.nn.relu(tf.matmul(obs, W1) + b1)\n",
    "    y1_new = tf.nn.relu(tf.matmul(obs_new, W1) + b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden_units, 2]))\n",
    "    b2 = tf.Variable(tf.random_normal([2]))\n",
    "\n",
    "    y2 = tf.matmul(y1, W2) + b2\n",
    "    y2_new = tf.matmul(y1_new, W2) + b2\n",
    "\n",
    "    return y2,y2_new\n",
    "\n",
    "\n",
    "\n",
    "def testing(tf_random_seed, np_random_seed, mode, learning_rate, batch_size, optimizer, num_hidden_units, render_flag):\n",
    "    print(tf_random_seed, np_random_seed, mode, learning_rate, batch_size, optimizer, num_hidden_units, render_flag)\n",
    "\n",
    "    # define model\n",
    "    y2,y2_new = define_model(tf_random_seed, num_hidden_units)\n",
    "    \n",
    "    # get action by Q values\n",
    "    action_max = tf.cast(tf.argmax(y2,axis=1),tf.int32)\n",
    "    \n",
    "    # calculate bellman loss\n",
    "    data_amount = tf.shape(y2_new)[0]\n",
    "    Q_old_index = tf.concat([tf.reshape(tf.range(0,limit=data_amount),[data_amount,1]), tf.reshape(act,[data_amount,1])],axis=1)\n",
    "    old_q = tf.gather_nd(y2,Q_old_index)\n",
    "    next_q_max = tf.reduce_max(y2_new, axis=1)\n",
    "\n",
    "    change = (reward_place + dis_factor * tf.stop_gradient(next_q_max) - old_q)\n",
    "    bellman_loss = tf.reduce_mean(tf.square(change)/2)\n",
    "\n",
    "    train_step =tf.train.RMSPropOptimizer(learning_rate=learning_rate,decay=0.9,momentum=0.2,centered=True).minimize(bellman_loss)\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        model_name = \"../models/A_5/A_5_6111_3338_non_linear_0.0001_128_RMS_30.checkpoint\"\n",
    "        print(model_name)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_name)\n",
    "\n",
    "        perf_list = []\n",
    "        dis_rtn_list = []\n",
    "        if render_flag:\n",
    "            test_length = 1\n",
    "        else:\n",
    "            test_length = 100\n",
    "\n",
    "        \n",
    "        for episode in range(test_length):\n",
    "            old_obs = env.reset()\n",
    "            dis_return = 0\n",
    "            dis_value = 1            \n",
    "            for length in range(1, 301):\n",
    "                if render_flag:\n",
    "                    env.render()\n",
    "                if length > 1 and np.random.uniform() > greedy_rate:\n",
    "                    action_dic = {obs:[old_obs],obs_new:np.zeros([1,4]),act:[0],reward_place:[reward]}\n",
    "                    action = action_max.eval(action_dic)[0]\n",
    "                else:\n",
    "                    action = round(np.random.uniform())\n",
    "\n",
    "                # get the observation, reward, and state of done, and information from the action\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                # set the reward to be 0 on non-terminating steps and -1 on termination\n",
    "                reward = -1 if done else 0\n",
    "\n",
    "                # count the number of action token\n",
    "                dis_return += dis_value * reward\n",
    "                dis_value *= dis_factor\n",
    "\n",
    "                old_obs = observation\n",
    "                if done or length == 300:\n",
    "                    perf_list.append(length)\n",
    "                    dis_rtn_list.append(dis_return)\n",
    "                    break\n",
    "        print()\n",
    "        print(\"mean of episode length = {}, mean of discounted returns = {}\".format(np.mean(perf_list), np.mean(dis_rtn_list)))\n",
    "        \n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    skip_index = []\n",
    "    testing(1, 1, \"non_linear\", 1e-4, 128, \"RMS\", 30, render_flag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A 5 1000 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# disable tensorflow debugging information\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# make the gym enviorment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# define some fixed hyperparameters\n",
    "dis_factor = 0.99\n",
    "greedy_rate = 0.05\n",
    "# get the file name for storing\n",
    "file_name = \"test\"\n",
    "\n",
    "# define some place holder\n",
    "reward_place = tf.placeholder(tf.float32, [None])\n",
    "obs_new = tf.placeholder(tf.float32, [None,4])\n",
    "obs = tf.placeholder(tf.float32, [None,4])\n",
    "act = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "\n",
    "# define the model\n",
    "def define_model(seed_num, num_hidden_units):\n",
    "    tf.set_random_seed(seed_num)\n",
    "    W1 = tf.Variable(tf.random_normal([4, num_hidden_units]))\n",
    "    b1 = tf.Variable(tf.random_normal([num_hidden_units]))\n",
    "\n",
    "    y1 = tf.nn.relu(tf.matmul(obs, W1) + b1)\n",
    "    y1_new = tf.nn.relu(tf.matmul(obs_new, W1) + b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden_units, 2]))\n",
    "    b2 = tf.Variable(tf.random_normal([2]))\n",
    "\n",
    "    y2 = tf.matmul(y1, W2) + b2\n",
    "    y2_new = tf.matmul(y1_new, W2) + b2\n",
    "\n",
    "    return y2,y2_new\n",
    "\n",
    "\n",
    "\n",
    "def testing(tf_random_seed, np_random_seed, mode, learning_rate, batch_size, optimizer, num_hidden_units, render_flag):\n",
    "    print(tf_random_seed, np_random_seed, mode, learning_rate, batch_size, optimizer, num_hidden_units, render_flag)\n",
    "\n",
    "    # define model\n",
    "    y2,y2_new = define_model(tf_random_seed, num_hidden_units)\n",
    "    \n",
    "    # get action by Q values\n",
    "    action_max = tf.cast(tf.argmax(y2,axis=1),tf.int32)\n",
    "    \n",
    "    # calculate bellman loss\n",
    "    data_amount = tf.shape(y2_new)[0]\n",
    "    Q_old_index = tf.concat([tf.reshape(tf.range(0,limit=data_amount),[data_amount,1]), tf.reshape(act,[data_amount,1])],axis=1)\n",
    "    old_q = tf.gather_nd(y2,Q_old_index)\n",
    "    next_q_max = tf.reduce_max(y2_new, axis=1)\n",
    "\n",
    "    change = (reward_place + dis_factor * tf.stop_gradient(next_q_max) - old_q)\n",
    "    bellman_loss = tf.reduce_mean(tf.square(change)/2)\n",
    "\n",
    "    train_step =tf.train.RMSPropOptimizer(learning_rate=learning_rate,decay=0.9,momentum=0.2,centered=True).minimize(bellman_loss)\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        model_name = \"../models/A_5/A_5_962_192_non_linear_0.0001_128_RMS_1000.checkpoint\"\n",
    "        print(model_name)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_name)\n",
    "\n",
    "        perf_list = []\n",
    "        dis_rtn_list = []\n",
    "        if render_flag:\n",
    "            test_length = 1\n",
    "        else:\n",
    "            test_length = 100\n",
    "\n",
    "        \n",
    "        for episode in range(test_length):\n",
    "            old_obs = env.reset()\n",
    "            dis_return = 0\n",
    "            dis_value = 1            \n",
    "            for length in range(1, 301):\n",
    "                if render_flag:\n",
    "                    env.render()\n",
    "                if length > 1 and np.random.uniform() > greedy_rate:\n",
    "                    action_dic = {obs:[old_obs],obs_new:np.zeros([1,4]),act:[0],reward_place:[reward]}\n",
    "                    action = action_max.eval(action_dic)[0]\n",
    "                else:\n",
    "                    action = round(np.random.uniform())\n",
    "\n",
    "                # get the observation, reward, and state of done, and information from the action\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                # set the reward to be 0 on non-terminating steps and -1 on termination\n",
    "                reward = -1 if done else 0\n",
    "\n",
    "                # count the number of action token\n",
    "                dis_return += dis_value * reward\n",
    "                dis_value *= dis_factor\n",
    "\n",
    "                old_obs = observation\n",
    "                if done or length == 300:\n",
    "                    perf_list.append(length)\n",
    "                    dis_rtn_list.append(dis_return)\n",
    "                    break\n",
    "        print()\n",
    "        print(\"mean of episode length = {}, mean of discounted returns = {}\".format(np.mean(perf_list), np.mean(dis_rtn_list)))\n",
    "        \n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    skip_index = []\n",
    "    testing(1, 1, \"non_linear\", 1e-4, 128, \"RMS\", 1000, render_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# disable tensorflow debugging information\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# make the gym enviorment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# define some fixed hyperparameters\n",
    "dis_factor = 0.99\n",
    "greedy_rate = 0.05\n",
    "# get the file name for storing\n",
    "file_name = \"test\"\n",
    "\n",
    "# define some place holder\n",
    "reward_place = tf.placeholder(tf.float32, [None])\n",
    "obs_new = tf.placeholder(tf.float32, [None,4])\n",
    "obs = tf.placeholder(tf.float32, [None,4])\n",
    "act = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "\n",
    "# define the model\n",
    "def define_model(seed_num, num_hidden_units):\n",
    "    tf.set_random_seed(seed_num)\n",
    "    W1 = tf.Variable(tf.random_normal([4, num_hidden_units]))\n",
    "    b1 = tf.Variable(tf.random_normal([num_hidden_units]))\n",
    "\n",
    "    y1 = tf.nn.relu(tf.matmul(obs, W1) + b1)\n",
    "    y1_new = tf.nn.relu(tf.matmul(obs_new, W1) + b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden_units, 2]))\n",
    "    b2 = tf.Variable(tf.random_normal([2]))\n",
    "\n",
    "    y2 = tf.matmul(y1, W2) + b2\n",
    "    y2_new = tf.matmul(y1_new, W2) + b2\n",
    "\n",
    "    return y2,y2_new\n",
    "\n",
    "\n",
    "\n",
    "def testing(tf_random_seed, np_random_seed, mode, learning_rate, batch_size, optimizer, num_hidden_units, render_flag):\n",
    "    print(tf_random_seed, np_random_seed, mode, learning_rate, batch_size, optimizer, num_hidden_units, render_flag)\n",
    "\n",
    "\n",
    "    # define model\n",
    "    y2,y2_new = define_model(tf_random_seed, num_hidden_units)\n",
    "    \n",
    "    # get action by Q values\n",
    "    action_max = tf.cast(tf.argmax(y2,axis=1),tf.int32)\n",
    "    \n",
    "    # calculate bellman loss\n",
    "    data_amount = tf.shape(y2_new)[0]\n",
    "    Q_old_index = tf.concat([tf.reshape(tf.range(0,limit=data_amount),[data_amount,1]), tf.reshape(act,[data_amount,1])],axis=1)\n",
    "    old_q = tf.gather_nd(y2,Q_old_index)\n",
    "    next_q_max = tf.reduce_max(y2_new, axis=1)\n",
    "\n",
    "    change = (reward_place + dis_factor * tf.stop_gradient(next_q_max) - old_q)\n",
    "    bellman_loss = tf.reduce_mean(tf.square(change)/2)\n",
    "\n",
    "    train_step =tf.train.RMSPropOptimizer(learning_rate=learning_rate,decay=0.9,momentum=0.2,centered=True).minimize(bellman_loss)\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        model_name = \"../models/A_6/A_6_84_9268_non_linear_0.0001_128_RMS.checkpoint\"\n",
    "        print(model_name)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_name)\n",
    "\n",
    "        perf_list = []\n",
    "        dis_rtn_list = []\n",
    "        \n",
    "        if render_flag:\n",
    "            test_length = 1\n",
    "        else:\n",
    "            test_length = 100\n",
    "            \n",
    "        for episode in range(test_length):\n",
    "            old_obs = env.reset()\n",
    "            dis_return = 0\n",
    "            dis_value = 1            \n",
    "            for length in range(1, 301):\n",
    "                if render_flag:\n",
    "                    env.render()\n",
    "                if length > 1 and np.random.uniform() > greedy_rate:\n",
    "                    action_dic = {obs:[old_obs],obs_new:np.zeros([1,4]),act:[0],reward_place:[reward]}\n",
    "                    action = action_max.eval(action_dic)[0]\n",
    "                else:\n",
    "                    action = round(np.random.uniform())\n",
    "\n",
    "                # get the observation, reward, and state of done, and information from the action\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                # set the reward to be 0 on non-terminating steps and -1 on termination\n",
    "                reward = -1 if done else 0\n",
    "\n",
    "                # count the number of action token\n",
    "                dis_return += dis_value * reward\n",
    "                dis_value *= dis_factor\n",
    "\n",
    "                old_obs = observation\n",
    "                if done or length == 300:\n",
    "                    perf_list.append(length)\n",
    "                    dis_rtn_list.append(dis_return)\n",
    "                    break\n",
    "        print()\n",
    "        print(\"mean of episode length = {}, mean of discounted returns = {}\".format(np.mean(perf_list), np.mean(dis_rtn_list)))\n",
    "        \n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    skip_index = []\n",
    "    testing(1, 1, \"non_linear\", 1e-4, 128, \"RMS\", 100, render_flag)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# disable tensorflow debugging information\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# make the gym enviorment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# define some fixed hyperparameters\n",
    "dis_factor = 0.99\n",
    "greedy_rate = 0.05\n",
    "# get the file name for storing\n",
    "file_name = \"test\"\n",
    "\n",
    "# define some place holder\n",
    "reward_place = tf.placeholder(tf.float32, [None])\n",
    "obs_new = tf.placeholder(tf.float32, [None,4])\n",
    "obs = tf.placeholder(tf.float32, [None,4])\n",
    "act = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "\n",
    "# define the model\n",
    "def define_model(seed_num, num_hidden_units):\n",
    "    tf.set_random_seed(seed_num)\n",
    "    W1 = tf.Variable(tf.random_normal([4, num_hidden_units]))\n",
    "    b1 = tf.Variable(tf.random_normal([num_hidden_units]))\n",
    "\n",
    "    y1 = tf.nn.relu(tf.matmul(obs, W1) + b1)\n",
    "    y1_new = tf.nn.relu(tf.matmul(obs_new, W1) + b1)\n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden_units, 2]))\n",
    "    b2 = tf.Variable(tf.random_normal([2]))\n",
    "\n",
    "    y2 = tf.matmul(y1, W2) + b2\n",
    "    y2_new = tf.matmul(y1_new, W2) + b2\n",
    "\n",
    "    # define a target network to copy to\n",
    "    W1_target = tf.Variable(tf.random_normal([4, num_hidden_units]))\n",
    "    b1_target = tf.Variable(tf.random_normal([num_hidden_units]))\n",
    "\n",
    "    y1_target = tf.nn.relu(tf.matmul(obs, W1_target) + b1_target)\n",
    "    y1_target_new = tf.nn.relu(tf.matmul(obs_new, W1_target) + b1_target)\n",
    "    W2_target = tf.Variable(tf.random_normal([num_hidden_units, 2]))\n",
    "    b2_target = tf.Variable(tf.random_normal([2]))\n",
    "\n",
    "    y2_target = tf.matmul(y1_target, W2_target) + b2_target\n",
    "    y2_target_new = tf.matmul(y1_target_new, W2_target) + b2_target\n",
    "\n",
    "    # define some variables to update the model\n",
    "    global update_1, update_2, update_3, update_4\n",
    "\n",
    "    update_1 = W1.assign(W1_target)\n",
    "    update_2 = b1.assign(b1_target)\n",
    "    update_3 = W2.assign(W2_target)\n",
    "    update_4 = b2.assign(b2_target)\n",
    "\n",
    "    return y2,y2_new, y2_target, y2_target_new\n",
    "\n",
    "# function to update the model\n",
    "def update(sess):\n",
    "    sess.run(update_1)\n",
    "    sess.run(update_2)\n",
    "    sess.run(update_3)\n",
    "    sess.run(update_4)\n",
    "\n",
    "def testing(tf_random_seed, np_random_seed, mode, learning_rate, batch_size, optimizer, num_hidden_units, render_flag):\n",
    "    print(tf_random_seed, np_random_seed, mode, learning_rate, batch_size, optimizer, num_hidden_units, render_flag)\n",
    "\n",
    "\n",
    "    # define model\n",
    "    y2, y2_new, y2_target, y2_target_new= define_model(tf_random_seed, num_hidden_units)\n",
    "    # get action by Q values\n",
    "    action_max = tf.cast(tf.argmax(y2, axis=1), tf.int32)\n",
    "    # calculate bellman loss\n",
    "    data_amount = tf.shape(y2_new)[0]\n",
    "    Q_old_index = tf.concat([tf.reshape(tf.range(0,limit=data_amount),[data_amount,1]), tf.reshape(act,[data_amount,1])],axis=1)\n",
    "    old_q = tf.gather_nd(y2,Q_old_index)\n",
    "    next_q_max = tf.reduce_max(y2_target_new, axis=1)\n",
    "\n",
    "    change = (reward_place + dis_factor * tf.stop_gradient(next_q_max) - old_q)\n",
    "    bellman_loss = tf.reduce_mean(tf.square(change)/2)\n",
    "\n",
    "    train_step =tf.train.RMSPropOptimizer(learning_rate=learning_rate,decay=0.9,momentum=0.2,centered=True).minimize(bellman_loss)\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        model_name = \"../models/A_7/A_7_4798_5097_non_linear_0.0001_128_RMS.checkpoint\"\n",
    "        print(model_name)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_name)\n",
    "\n",
    "        perf_list = []\n",
    "        dis_rtn_list = []\n",
    "        \n",
    "        if render_flag:\n",
    "            test_length = 1\n",
    "        else:\n",
    "            test_length = 100\n",
    "            \n",
    "        for episode in range(test_length):\n",
    "            old_obs = env.reset()\n",
    "            dis_return = 0\n",
    "            dis_value = 1            \n",
    "            for length in range(1, 301):\n",
    "                if render_flag:\n",
    "                    env.render()\n",
    "                if length > 1 and np.random.uniform() > greedy_rate:\n",
    "                    action_dic = {obs:[old_obs],obs_new:np.zeros([1,4]),act:[0],reward_place:[reward]}\n",
    "                    action = action_max.eval(action_dic)[0]\n",
    "                else:\n",
    "                    action = round(np.random.uniform())\n",
    "\n",
    "                # get the observation, reward, and state of done, and information from the action\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                # set the reward to be 0 on non-terminating steps and -1 on termination\n",
    "                reward = -1 if done else 0\n",
    "\n",
    "                # count the number of action token\n",
    "                dis_return += dis_value * reward\n",
    "                dis_value *= dis_factor\n",
    "\n",
    "                old_obs = observation\n",
    "                if done or length == 300:\n",
    "                    perf_list.append(length)\n",
    "                    dis_rtn_list.append(dis_return)\n",
    "                    break\n",
    "        print()\n",
    "        print(\"mean of episode length = {}, mean of discounted returns = {}\".format(np.mean(perf_list), np.mean(dis_rtn_list)))\n",
    "        \n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    skip_index = []\n",
    "    testing(1, 1, \"non_linear\", 1e-4, 128, \"RMS\", 100, render_flag)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# disable tensorflow debugging information\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# make the gym enviorment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# define some fixed hyperparameters\n",
    "dis_factor = 0.99\n",
    "greedy_rate = 0.05\n",
    "# get the file name for storing\n",
    "file_name = \"test\"\n",
    "\n",
    "# define some place holder\n",
    "reward_place = tf.placeholder(tf.float32, [None])\n",
    "obs_new = tf.placeholder(tf.float32, [None,4])\n",
    "obs = tf.placeholder(tf.float32, [None,4])\n",
    "act = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "\n",
    "# define the model\n",
    "def define_model(seed_num, num_hidden_units):\n",
    "    tf.set_random_seed(seed_num)\n",
    "    W1 = tf.Variable(tf.random_normal([4, num_hidden_units]))\n",
    "    b1 = tf.Variable(tf.random_normal([num_hidden_units]))\n",
    "\n",
    "    y1 = tf.nn.relu(tf.matmul(obs, W1) + b1)\n",
    "    y1_new = tf.nn.relu(tf.matmul(obs_new, W1) + b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden_units, 2]))\n",
    "    b2 = tf.Variable(tf.random_normal([2]))\n",
    "\n",
    "    y2 = tf.matmul(y1, W2) + b2\n",
    "    y2_new = tf.matmul(y1_new, W2) + b2\n",
    "\n",
    "    # define a target network to copy to\n",
    "    W1_target = tf.Variable(tf.random_normal([4, num_hidden_units]))\n",
    "    b1_target = tf.Variable(tf.random_normal([num_hidden_units]))\n",
    "\n",
    "    y1_target = tf.nn.relu(tf.matmul(obs, W1_target) + b1_target)\n",
    "    y1_target_new = tf.nn.relu(tf.matmul(obs_new, W1_target) + b1_target)\n",
    "\n",
    "    W2_target = tf.Variable(tf.random_normal([num_hidden_units, 2]))\n",
    "    b2_target = tf.Variable(tf.random_normal([2]))\n",
    "\n",
    "    y2_target = tf.matmul(y1_target, W2_target) + b2_target\n",
    "    y2_target_new = tf.matmul(y1_target_new, W2_target) + b2_target\n",
    "\n",
    "    # define some variables to update the model\n",
    "    global update_1, update_2, update_3, update_4\n",
    "\n",
    "    update_1 = W1.assign(W1_target)\n",
    "    update_2 = b1.assign(b1_target)\n",
    "    update_3 = W2.assign(W2_target)\n",
    "    update_4 = b2.assign(b2_target)\n",
    "\n",
    "    return y2,y2_new, y2_target, y2_target_new\n",
    "\n",
    "# function to update the model\n",
    "def update(sess):\n",
    "    sess.run(update_1)\n",
    "    sess.run(update_2)\n",
    "    sess.run(update_3)\n",
    "    sess.run(update_4)\n",
    "\n",
    "def testing(tf_random_seed, np_random_seed, mode, learning_rate, batch_size, optimizer, num_hidden_units, render_flag):\n",
    "    print(tf_random_seed, np_random_seed, mode, learning_rate, batch_size, optimizer, num_hidden_units, render_flag)\n",
    "\n",
    "\n",
    "    # define model\n",
    "    y2, y2_new, y2_target, y2_target_new= define_model(tf_random_seed, num_hidden_units)\n",
    "    # get action by Q values\n",
    "    action_max = tf.cast(tf.argmax(y2, axis=1), tf.int32)\n",
    "    # calculate bellman loss\n",
    "    data_amount = tf.shape(y2_new)[0]\n",
    "    Q_old_index = tf.concat([tf.reshape(tf.range(0,limit=data_amount),[data_amount,1]), tf.reshape(act,[data_amount,1])],axis=1)\n",
    "    old_q = tf.gather_nd(y2,Q_old_index)\n",
    "\n",
    "    action_max_primary = tf.cast(tf.argmax(y2_new, axis=1), tf.int32)\n",
    "    Q_new_index = tf.concat([tf.reshape(tf.range(0,limit=data_amount),[data_amount,1]), tf.reshape(action_max_primary,[data_amount,1])],axis=1)\n",
    "    \n",
    "    next_q_max = tf.gather_nd(y2_target_new, Q_new_index)\n",
    "\n",
    "    change = (reward_place + dis_factor * tf.stop_gradient(next_q_max) - old_q)\n",
    "    bellman_loss = tf.reduce_mean(tf.square(change)/2)\n",
    "\n",
    "    train_step =tf.train.RMSPropOptimizer(learning_rate=learning_rate,decay=0.9,momentum=0.2,centered=True).minimize(bellman_loss)\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        model_name = \"../models/A_8/A_8_3621_6221_non_linear_0.0001_128_RMS.checkpoint\"\n",
    "        print(model_name)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_name)\n",
    "\n",
    "        perf_list = []\n",
    "        dis_rtn_list = []\n",
    "        \n",
    "        if render_flag:\n",
    "            test_length = 1\n",
    "        else:\n",
    "            test_length = 100        \n",
    "        \n",
    "        for episode in range(test_length):\n",
    "            old_obs = env.reset()\n",
    "            dis_return = 0\n",
    "            dis_value = 1            \n",
    "            for length in range(1, 301):\n",
    "                if render_flag:\n",
    "                    env.render()\n",
    "                if length > 1 and np.random.uniform() > greedy_rate:\n",
    "                    action_dic = {obs:[old_obs],obs_new:np.zeros([1,4]),act:[0],reward_place:[reward]}\n",
    "                    action = action_max.eval(action_dic)[0]\n",
    "                else:\n",
    "                    action = round(np.random.uniform())\n",
    "\n",
    "                # get the observation, reward, and state of done, and information from the action\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                # set the reward to be 0 on non-terminating steps and -1 on termination\n",
    "                reward = -1 if done else 0\n",
    "\n",
    "                # count the number of action token\n",
    "                dis_return += dis_value * reward\n",
    "                dis_value *= dis_factor\n",
    "\n",
    "                old_obs = observation\n",
    "                if done or length == 300:\n",
    "                    perf_list.append(length)\n",
    "                    dis_rtn_list.append(dis_return)\n",
    "                    break\n",
    "        print()\n",
    "        print(\"mean of episode length = {}, mean of discounted returns = {}\".format(np.mean(perf_list), np.mean(dis_rtn_list)))\n",
    "        \n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    skip_index = []\n",
    "    testing(1, 1, \"non_linear\", 1e-4, 128, \"RMS\", 100, render_flag)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
