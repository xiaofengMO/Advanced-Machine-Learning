{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "checking performace\n",
      "Finished in 00:01:06                                                                                            \n",
      "Pong True\n",
      "score_list mean -20.67\n",
      "score_list std 0.721872564931\n",
      "dis_return mean -1.05139944159\n",
      "dis_return std 0.235883016621\n",
      "frame_length mean 1106.14\n",
      "frame_length std 114.015176183\n",
      "\n",
      "checking performace\n",
      "Finished in 00:02:25                                                                                            \n",
      "Pong False\n",
      "score_list mean -20.85\n",
      "score_list std 0.384057287393\n",
      "dis_return mean -0.911045102377\n",
      "dis_return std 0.24007724723\n",
      "frame_length mean 1184.09\n",
      "frame_length std 120.317338318\n",
      "\n",
      "checking performace\n",
      "Finished in 00:00:42                                                                                            \n",
      "MsPacgirl True\n",
      "score_list mean 20.26\n",
      "score_list std 5.81828153324\n",
      "dis_return mean 2.63095301316\n",
      "dis_return std 0.590958102998\n",
      "frame_length mean 638.27\n",
      "frame_length std 76.7352402746\n",
      "\n",
      "checking performace\n",
      "Finished in 00:01:20                                                                                            \n",
      "MsPacgirl False\n",
      "score_list mean 25.92\n",
      "score_list std 6.70921754007\n",
      "dis_return mean 3.34080440979\n",
      "dis_return std 0.815875594724\n",
      "frame_length mean 636.8\n",
      "frame_length std 174.161247125\n",
      "\n",
      "checking performace\n",
      "Finished in 00:03:01                                                                                            \n",
      "Boxing True\n",
      "score_list mean 1.3\n",
      "score_list std 4.73392015142\n",
      "dis_return mean -0.24277197911\n",
      "dis_return std 1.05788204778\n",
      "frame_length mean 2381.61\n",
      "frame_length std 12.7623626339\n",
      "\n",
      "checking performace\n",
      "Finished in 00:05:32                                                                                            \n",
      "Boxing False\n",
      "score_list mean -15.21\n",
      "score_list std 7.12361565499\n",
      "dis_return mean -0.141179368055\n",
      "dis_return std 0.715475302094\n",
      "frame_length mean 2380.85\n",
      "frame_length std 12.5438231812\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "from statistics import mean, stdev\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "# import scipy\n",
    "# from scipy import misc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys,time\n",
    "import math\n",
    "import random\n",
    "# from XFLib_1 import *\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "\n",
    "# disable tensorflow debugging information\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# a function to estimate the time remaining, home-made, skip reading it\n",
    "class time_est():\n",
    "    def __init__(self, total_len):\n",
    "        self.t_start = time.time()\n",
    "        self.total_len = total_len\n",
    "        self.count = 0\n",
    "        self.t_ref = time.time()\n",
    "    \n",
    "    def check(self,no_of_check=1,info=\"\"):\n",
    "        self.count += no_of_check\n",
    "        if time.time() - self.t_ref > 1 and self.count > 0:\n",
    "            t_used = time.time() - self.t_start\n",
    "            t_total = t_used * self.total_len / self.count\n",
    "            t_remain = t_total - t_used\n",
    "            process_bar = \"|\"\n",
    "            for i in range(40):\n",
    "                if (i/40) < (self.count/self.total_len):\n",
    "                    process_bar += \"█\"\n",
    "                else:\n",
    "                    process_bar += \" \"\n",
    "            process_bar += \"|\"\n",
    "            if info != \"\":\n",
    "                info = str(info) + \"  \"\n",
    "            print(\"\\r\" + (str(info) + \"{:.2f}% ({}/{})  \".format(self.count * 100/self.total_len, self.count,self.total_len)) \n",
    "                  + str(process_bar).ljust(45) \n",
    "                  + \"Used: {:02.0f}:{:02.0f}:{:02.0f}\".format(int(t_used/3600), int(t_used/60)%60, t_used % 60).ljust(16) \n",
    "                  + \"ETA: {:02.0f}:{:02.0f}:{:02.0f}\".format(int(t_remain/3600), int(t_remain/60)%60, t_remain % 60),end=\"\")\n",
    "            self.t_ref = time.time()\n",
    "        if self.count == self.total_len:\n",
    "            t_used = time.time() - self.t_start\n",
    "            if info != \"\":\n",
    "                info = str(info) + \"  \"\n",
    "            print(\"\\r\" + str(info) + \"Finished in \" \n",
    "                  + \"{:02.0f}:{:02.0f}:{:02.0f}\".format(int(t_used/3600), int(t_used/60)%60, t_used % 60).ljust(100))\n",
    "    def get(self,no_of_check=1):\n",
    "        process_bar = \"|\"\n",
    "        for i in range(40):\n",
    "            if (i/40) < (self.count/self.total_len):\n",
    "                process_bar += \"█\"\n",
    "            else:\n",
    "                process_bar += \" \"\n",
    "        process_bar += \"|\"\n",
    "        self.count += no_of_check\n",
    "        t_used = time.time() - self.t_start\n",
    "        t_total = t_used * self.total_len / self.count\n",
    "        t_remain = t_total - t_used\n",
    "        return \"{} ETA: {:02.0f}:{:02.0f}:{:02.0f}\".format(process_bar, int(t_remain/3600), int(t_remain/60)%60, t_remain % 60)\n",
    "\n",
    "\n",
    "def test_progress(game_name, random_flag):\n",
    "\n",
    "    # choose the size of buffer\n",
    "    buffer_size = 500000\n",
    "\n",
    "    # setup hyperparameter with respect to different games\n",
    "    if game_name == 'Pong':\n",
    "        env = gym.make(\"Pong-v4\")\n",
    "        env_1 = gym.make(\"Pong-v4\")\n",
    "        num_of_actions = 6\n",
    "        max_runs = 2000000\n",
    "        Learning_rate = 1e-3 # 1e-7\n",
    "        final_greedy_rate = 0.1\n",
    "        greedy_stop_num = 600000\n",
    "        discount = .99\n",
    "        greedy_stop_number = max_runs * 10\n",
    "        train_start = 1\n",
    "        eval_start  = 100000\n",
    "        render_flag = 0\n",
    "        optimizer = \"RMS\"\n",
    "        test_threshold = -18\n",
    "        save_threshold = -21\n",
    "    elif game_name == 'MsPacgirl':\n",
    "        env = gym.make(\"MsPacman-v4\")\n",
    "        env_1 = gym.make(\"MsPacman-v4\")\n",
    "        num_of_actions = 9\n",
    "        max_runs = 2000000\n",
    "        Learning_rate = 1e-3 # 1e-7\n",
    "        final_greedy_rate = 0.1\n",
    "        greedy_stop_num = 600000\n",
    "        discount = .99\n",
    "        greedy_stop_number = max_runs * 10\n",
    "        train_start = 1\n",
    "        eval_start  = 100000\n",
    "        render_flag = 0\n",
    "        optimizer = \"RMS\"\n",
    "        test_threshold = 20\n",
    "        save_threshold = 20\n",
    "    elif game_name == 'Boxing':\n",
    "        env = gym.make(\"Boxing-v4\")\n",
    "        env_1 = gym.make(\"Boxing-v4\")\n",
    "        num_of_actions = 18\n",
    "        max_runs = 2000000\n",
    "        Learning_rate = 1e-3 # 1e-7\n",
    "        final_greedy_rate = 0.1\n",
    "        greedy_stop_num = 600000\n",
    "        discount = .99\n",
    "        greedy_stop_number = max_runs * 10\n",
    "        train_start = 1\n",
    "        eval_start  = 100000\n",
    "        render_flag = 0\n",
    "        optimizer = \"RMS\"\n",
    "        test_threshold = -10\n",
    "        save_threshold = -10\n",
    "    else:\n",
    "        raise ValueError('Unidentified game mode')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # a function to convert image from rgd to grey, since color is not useful, a grey image can simplify\n",
    "    def rgb2gray(rgb):\n",
    "        r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "        gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "        return gray\n",
    "\n",
    "    # some hyperparameters\n",
    "    loss_set = []\n",
    "    batch_size = 20\n",
    "    #hyperparameters for CNN\n",
    "    width = 6\n",
    "    height = 6\n",
    "    layer_1_unit = 16\n",
    "    layer_2_unit = 32\n",
    "\n",
    "    #size of the image\n",
    "    image_size = 28\n",
    "\n",
    "    reward_place = tf.placeholder(tf.float32, [None])\n",
    "    obs_new = tf.placeholder(tf.float32, [None,image_size,image_size,4])\n",
    "    obs = tf.placeholder(tf.float32, [None,image_size,image_size,4])\n",
    "    act = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "\n",
    "    # two dimensional convolutional\n",
    "    def convolution2D(DataIn,Weight):\n",
    "        return tf.nn.conv2d(DataIn,Weight,strides = [1,2,2,1], padding = 'SAME')         \n",
    "\n",
    "    # a function to define the neural network model\n",
    "    def define_model(seed_num):\n",
    "        tf.set_random_seed(seed_num)\n",
    "        W1 = tf.Variable(tf.random_normal([width, height, 4, layer_1_unit]))\n",
    "        b1 = tf.Variable(tf.random_normal([layer_1_unit]))\n",
    "\n",
    "        y1 = tf.nn.relu(convolution2D(obs,W1)+b1)\n",
    "        y1_new = tf.nn.relu(convolution2D(obs_new,W1)+b1)\n",
    "\n",
    "        W2 = tf.Variable(tf.random_normal([width, height, layer_1_unit, layer_2_unit]))\n",
    "        b2 = tf.Variable(tf.random_normal([layer_2_unit]))\n",
    "\n",
    "        y2 = tf.nn.relu(convolution2D(y1,W2)+b2)\n",
    "        y2_new = tf.nn.relu(convolution2D(y1_new,W2)+b2)\n",
    "\n",
    "        # flatten the tensor before put into the fully connected layer\n",
    "        flatten_shape = int(image_size/4)*int(image_size/4)*layer_2_unit\n",
    "        W3 = tf.Variable(tf.random_normal([flatten_shape,256]))\n",
    "        b3 = tf.Variable(tf.random_normal([256]))\n",
    "\n",
    "        y3 = tf.nn.relu(tf.matmul(tf.reshape(y2,[-1,flatten_shape]),W3)+b3)\n",
    "        y3_new = tf.nn.relu(tf.matmul(tf.reshape(y2_new,[-1,flatten_shape]),W3)+b3)\n",
    "\n",
    "        W4 = tf.Variable(tf.random_normal([256,num_of_actions]))\n",
    "        b4 = tf.Variable(tf.random_normal([num_of_actions]))\n",
    "\n",
    "        y4 = tf.matmul(y3,W4) + b4\n",
    "        y4_new = tf.matmul(y3_new,W4) + b4\n",
    "\n",
    "        # target network which has the same structure, to copy the previous network\n",
    "        W1_target = tf.Variable(tf.random_normal([width, height, 4, layer_1_unit]))\n",
    "        b1_target = tf.Variable(tf.random_normal([layer_1_unit]))\n",
    "\n",
    "        y1_target = tf.nn.relu(convolution2D(obs, W1_target) + b1_target)\n",
    "        y1_new_target = tf.nn.relu(convolution2D(obs_new, W1_target) + b1_target)\n",
    "\n",
    "        W2_target = tf.Variable(\n",
    "            tf.random_normal([width, height, layer_1_unit, layer_2_unit]))\n",
    "        b2_target = tf.Variable(tf.random_normal([layer_2_unit]))\n",
    "\n",
    "        y2_target = tf.nn.relu(convolution2D(y1_target, W2_target) + b2_target)\n",
    "        y2_new_target = tf.nn.relu(convolution2D(y1_new_target, W2_target) + b2_target)\n",
    "\n",
    "        flatten_shape = int(image_size/4)*int(image_size/4)*layer_2_unit\n",
    "        W3_target = tf.Variable(tf.random_normal([flatten_shape, 256]))\n",
    "        b3_target = tf.Variable(tf.random_normal([256]))\n",
    "\n",
    "        y3_target = tf.nn.relu(tf.matmul(tf.reshape(y2_target, [-1, flatten_shape]), W3_target) + b3_target)\n",
    "        y3_new_target = tf.nn.relu(tf.matmul(tf.reshape(y2_new_target, [-1, flatten_shape]), W3_target) + b3_target)\n",
    "\n",
    "        W4_target = tf.Variable(tf.random_normal([256, num_of_actions]))\n",
    "        b4_target = tf.Variable(tf.random_normal([num_of_actions]))\n",
    "\n",
    "        y4_target = tf.matmul(y3_target, W4_target) + b4_target\n",
    "        y4_new_target = tf.matmul(y3_new_target, W4_target) + b4_target\n",
    "\n",
    "        # global same variables for updating the target network\n",
    "        global update_1, update_2, update_3, update_4, update_5, update_6, update_7, update_8\n",
    "\n",
    "        # variables to update the network\n",
    "        update_1 = W1_target.assign(W1)\n",
    "        update_2 = b1_target.assign(b1)\n",
    "        update_3 = W2_target.assign(W2)\n",
    "        update_4 = b2_target.assign(b2)\n",
    "        update_5 = W3_target.assign(W3)\n",
    "        update_6 = b3_target.assign(b3)\n",
    "        update_7 = W4_target.assign(W4)\n",
    "        update_8 = b4_target.assign(b4)\n",
    "\n",
    "        return y4,y4_new, y4_target, y4_new_target\n",
    "\n",
    "    # a function to update the target network\n",
    "    def update(sess):\n",
    "        sess.run(update_1)\n",
    "        sess.run(update_2)\n",
    "        sess.run(update_3)\n",
    "        sess.run(update_4)\n",
    "        sess.run(update_5)\n",
    "        sess.run(update_6)\n",
    "        sess.run(update_7)\n",
    "        sess.run(update_8)\n",
    "\n",
    "\n",
    "    def image_preprocess(image):\n",
    "        # if the image is Pong, cut the image so that the noisy part do not affect the network\n",
    "        if game_name == 'Pong':\n",
    "            image = image[34:194,:]\n",
    "        # process image to 28 by 28 grey image\n",
    "        img = Image.fromarray(image, 'RGB').convert('L')\n",
    "        img = img.resize((image_size,image_size),resample=Image.BILINEAR)\n",
    "        image_trans = np.asarray(img, dtype=np.uint8)\n",
    "        # binarize the image of pong to just two level for network to better understand the image\n",
    "        if game_name == 'Pong':\n",
    "            image_trans.setflags(write=True)\n",
    "            image_trans[image_trans>90] = 255\n",
    "            image_trans[image_trans<=90] = 0\n",
    "\n",
    "        return image_trans\n",
    "\n",
    "    # a function to run the game with the current model for 100 times and average the performance\n",
    "    def test_process(sess, action_max, random_flag=True):\n",
    "        print(\"\\nchecking performace\")\n",
    "\n",
    "        est = time_est(100)\n",
    "        with sess.as_default():\n",
    "            # lists to store the the episode length and returns\n",
    "            score_list = []\n",
    "            frame_length = []\n",
    "            dis_return = []\n",
    "            for i in range(100):\n",
    "                old_obs = env_1.reset()\n",
    "                t = 0\n",
    "                frame_buffer = deque(maxlen=4)\n",
    "                total_reward = 0\n",
    "                discount_factor = 1\n",
    "                discounted_value = 0\n",
    "                count = 0\n",
    "                observation_stack = []\n",
    "\n",
    "                while 1:\n",
    "                    if count!= 0:\n",
    "                        discount_factor *= 0.99\n",
    "                    # greedy policy\n",
    "                    if t > 3 and random.random() > final_greedy_rate and random_flag != True:\n",
    "                        action_dic = {obs:[old_obs],obs_new:np.zeros([1,image_size,image_size,4]),act:[0],reward_place:[reward]}\n",
    "                        action = action_max.eval(action_dic)[0]\n",
    "                    else:\n",
    "                        action = round(random.uniform(0, num_of_actions - 1))\n",
    "                    # take action and get the response from the action\n",
    "                    observation, reward, done, info = env_1.step(action)\n",
    "                    obs_frame = image_preprocess(observation)\n",
    "                    frame_buffer.append(obs_frame)\n",
    "\n",
    "                    if len(frame_buffer) == 4:\n",
    "                        observation_stack = np.stack(list(frame_buffer),axis=0)\n",
    "                        observation_stack = observation_stack.transpose([1,2,0])\n",
    "                    # reward either -1, 1 or 0\n",
    "                    reward = np.clip(reward,-1,1)\n",
    "                    total_reward += reward\n",
    "                    count += 1\n",
    "                    # save the observation to a variable\n",
    "                    old_obs = observation_stack\n",
    "                    t += 1\n",
    "                    # calculate the discounted returns\n",
    "                    discounted_value += discount_factor * reward\n",
    "\n",
    "                    if done:\n",
    "                        # saving the reward and discouned rewards and frame count into list\n",
    "                        score_list.append(total_reward)\n",
    "                        frame_length.append(t)\n",
    "                        dis_return.append(discounted_value)\n",
    "                        break\n",
    "                est.check()\n",
    "        # return the mean of episode length and discounted rewards\n",
    "        return score_list, frame_length, dis_return\n",
    "\n",
    "    # function for training\n",
    "    def training(seed_num, random_flag):\n",
    "        # define the greedy rate, initially 1, it can be changed in the training\n",
    "        greedy_rate = 1\n",
    "\n",
    "        # define the highes reward to be -21, it can be changed in the training\n",
    "        highest_reward = -21\n",
    "\n",
    "        # define model\n",
    "        y2, y2_new, y2_target, y2_target_new= define_model(seed_num)\n",
    "        # get action by Q values\n",
    "        action_max = tf.cast(tf.argmax(y2, axis=1), tf.int32)\n",
    "        # calculate bellman loss\n",
    "        data_amount = tf.shape(y2_new)[0]\n",
    "        Q_old_index = tf.concat([tf.reshape(tf.range(0,limit=data_amount),[data_amount,1]), tf.reshape(act,[data_amount,1])],axis=1)\n",
    "        old_q = tf.gather_nd(y2,Q_old_index)\n",
    "        max_q_value_next = tf.reduce_max(y2_target_new, axis=1)\n",
    "\n",
    "        change_in_q = (reward_place + discount * tf.stop_gradient(max_q_value_next) - old_q)\n",
    "\n",
    "        # define the loss to be bellman loss\n",
    "        loss = tf.reduce_mean(tf.square(change_in_q)/2)\n",
    "\n",
    "        # optimizer can be changed by the setting\n",
    "        if optimizer == \"SGD\":\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate=Learning_rate).minimize(loss)\n",
    "        elif optimizer == \"ADAM\":\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=Learning_rate).minimize(loss)\n",
    "        elif optimizer == \"RMS\":\n",
    "            train_step = tf.train.RMSPropOptimizer(learning_rate=Learning_rate,decay=0.9,momentum=0.2,centered=True).minimize(loss)\n",
    "        else:\n",
    "            aaaaaaaaa\n",
    "\n",
    "\n",
    "        # here comes the test\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            score_list, frame_length, dis_return = test_process(sess, action_max, random_flag=random_flag)\n",
    "            print(game_name, random_flag)\n",
    "            print(\"score_list mean\", np.mean(score_list))\n",
    "            print(\"score_list std\", np.std(score_list))\n",
    "            print(\"dis_return mean\", np.mean(dis_return))\n",
    "            print(\"dis_return std\", np.std(dis_return))            \n",
    "            print(\"frame_length mean\", np.mean(frame_length))\n",
    "            print(\"frame_length std\", np.std(frame_length))\n",
    "            \n",
    "    training(round(random.uniform(0,3000)), random_flag)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for game_name in [\"Pong\", \"MsPacgirl\", \"Boxing\"]:\n",
    "        for random_flag in [True, False]:\n",
    "            test_progress(game_name, random_flag)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
